% Old 1.2 application...
An application of this method goes as follows: given a tabular healthcare dataset \( x \in \mathbb{R}^D \) (e.g., \( D = 100 \) lab features), we apply the Gaussianisation method. First, there is the 
Forward Transformation, where we apply a sequence of \( L \) Gaussianisation blocks:
$f_{\mathrm{block}}(x) = f_{\mathrm{dim}}(Qx),$ where \( Q \in O(D) \) is a random (or learned) orthogonal rotation matrix, and \( f_{\mathrm{dim}} \) consists of invertible one-dimensional non-linear transforms applied independently to each feature. The total loss which we want to minimise during this process is as follows:
\[
    \mathcal{L} = D_{\mathrm{KL}}(q(z) \, \| \, \mathcal{N}(0, I)) = D + \sum_{i=1}^D J_i,\]
    where \( D \) measures dimension interdependence and \( J_i \) is the marginal loss for each \( z_i \). After training, generate synthetic data by sampling \( z \sim \mathcal{N}(0, I) \) and applying the inverse transformation:
    \(
    x = f^{-1}(z).
    \)
    
This is all ultimately useful because synthetic patient data can be used for machine learning development, privacy-preserving data sharing, medical simulations, and robust outlier detection.


% Most recent 1.2
The Gaussian Property of Gaussianisation with Random Rotations makes it suitable for data compression, where the goal is to reduce redundancy in data representation to achieve efficient encoding.\cite{theiler2021distilled} 

A key property of an optimal compression system is that it maximises the entropy of the transformed data.\cite{theiler2021distilled} Given a dataset $X$, the entropy is given by:

\begin{equation}
H(X) = - \mathbb{E} [\log p(X)],
\end{equation}
where $p(X)$ is the probability density function of the data.

\noindent For lossless compression, the Shannon lower bound states that the minimum number of bits required to encode the data is given by:
$L_{\text{min}} = H(X)$.\cite{eecs_data_compression}


Gaussianisation reduces statistical dependencies by making the data more statistically independent through the iterative application of random rotations and non-linear transformations.\cite{johnson2020gaussianization} This process reduces correlations between dimensions, making the data more compactly representable.\cite{johnson2020gaussianization} 

Once Gaussianisation has been applied, the transformed data $Z = f(X)$ approximately follows a standard normal distribution $Z \sim \mathcal{N}(0, I)$. This allows for efficient arithmetic coding or Huffman coding since the transformed data follows a known and simple distribution. Huffman coding makes data smaller using the frequency with which codes appear in the data.\cite{BBC_Huffman_2025}

A real-world application of this is in genomics.


% Extra 1.2
To apply Gaussianisation to data compression, the process begins with preprocessing the data. Given high-dimensional data $X$, Gaussianisation is first applied to obtain a transformed representation $Z = f(X)$. This transformation ensures that the components of $Z$ are approximately independent and follow a standard normal distribution. After this step, quantisation is performed to achieve lossless compression. The transformed values $Z$ must be discretised (quantised) into a finite set of values. A common approach is uniform scalar quantisation with step size $\Delta$:

\begin{equation}
\hat{Z} = \text{round}(Z / \Delta) \cdot \Delta,
\end{equation}
where the choice of $\Delta$ affects the compression ratio and distortion.

Following quantisation, entropy coding is applied. The data is encoded using arithmetic coding or Huffman coding, leveraging the high entropy of the Gaussianised representation. On the decoder side, the inverse Gaussianisation transformation $X = f^{-1}(\hat{Z})$ is applied to reconstruct the original data.

The key quantity to optimise in this compression framework is the entropy of the transformed data, which is given by:

\begin{equation}
H(Z) = - \mathbb{E} [\log p(Z)],
\end{equation}
\noindent where $p(Z)$ is the density function of the Gaussianised representation. Since Gaussianisation makes $p(Z)$ approach a standard normal distribution, it maximises entropy while preserving invertibility, leading to an efficient encoding scheme.

Gaussianisation can be applied as a preprocessing step for lossless data compression. It transforms data into a form that is high entropy and statistically independent, making it more efficient for entropy-based encoding techniques. The approach aligns well with the theoretical properties discussed in the paper and leverages Gaussianisation’s ability to standardise and decorrelate data.



% Old 1.2
Maximising entropy is important because it relates to efficient input representation. You want to take data, such as images, determine their meaning, and compress it into a shorter category list.

This is a good thing because Gaussianisation reduces statistical dependencies. The paper shows that Gaussianisation makes the data more statistically independent by iteratively applying random rotations and non-linear transformations. This reduces correlations between dimensions, making the data more compactly representable. Secondly, transforming the data to a standard normal distribution maximises the entropy among all distributions with the same covariance structure. This makes the information content per bit optimal, which is crucial in compression. Thirdly. this facilitates entropy coding. After applying Gaussianisation, the transformed data $Z = f(X)$ approximately follows a standard normal distribution $Z \sim N(0, I)$. This allows for efficient arithmetic coding or Huffman coding since the transformed data follows a known and simple distribution.

The Method applies to this application because of the following. First, preprocess the data. Given high-dimensional data $X$, we first apply Gaussianisation to obtain a transformed representation $Z = f(X)$.
This transformation ensures that the components of $Z$ are approximately independent and follow a standard normal distribution.

Next, to achieve lossless compression, the transformed values $Z$ must be discretised (quantised) into a finite set of values. A common approach is uniform scalar quantisation with step size $\Delta$:
    \begin{equation}
    \hat{Z} = \text{round}(Z / \Delta) \cdot \Delta,
    \end{equation}
\noindent where the choice of $\Delta$ affects the compression ratio and distortion.

After quantisation, the data can be encoded using arithmetic coding or Huffman coding, leveraging the high entropy of the Gaussianised representation.

On the decoder side, the inverse Gaussianisation transformation $X = f^{-1}(\hat{Z})$ is applied to reconstruct the original data.

The key quantity to optimise in this compression framework is the entropy of the transformed data:
\begin{equation}
H(Z) = - \mathbb{E} [\log p(Z)],
\end{equation}

\noindent where $p(Z)$ is the density function of the Gaussianised representation.

Since Gaussianisation makes $p(Z)$ approach a standard normal distribution, it maximises entropy while preserving invertibility, leading to an efficient encoding scheme.

Gaussianisation can be applied as a preprocessing step for lossless data compression by transforming data into a form that is high entropy and statistically independent, making it more efficient for entropy-based encoding techniques. This application aligns well with the theoretical properties discussed in the paper and leverages Gaussianisation’s ability to standardise and decorrelate data.

% Initial Answer

Gaussianization is a form of normalizing flow that iteratively transforms data into a Gaussian (normal) distribution using a sequence of invertible \textit{rotation+nonlinear} layers \cite{draxler2023convergence}. Each layer applies a random rotation (an orthogonal linear transformation), followed by a Gaussianization step where each dimension undergoes an independent nonlinear transformation to make its marginal distribution approach a standard normal. This approach allows for training without backpropagation (e.g., by fitting one-dimensional empirical cumulative distribution functions), and it has demonstrated effectiveness on low-dimensional data. However, as the dimensionality $D$ increases, convergence slows significantly, an issue that this paper investigates in depth. The main contributions of the paper are as follows:

\begin{itemize}
    \item \textbf{Analytical Scaling Result}: The authors provide explicit proof that the number of layers $L$ required to achieve a given improvement in Kullback-Leibler (KL) divergence grows \textit{linearly} with the dimension $D$. Under mild assumptions (centred data with normalised covariance), they establish that at least $L = \Omega(D)$ layers are necessary for the worst-case scenario \cite{draxler2023convergence}. This result explains the observed slowdown, showing that higher-dimensional distributions require proportionally more layers to reach the same level of Gaussianisation quality.
    
    \item \textbf{Limitations of Learning Rotations}: The authors show that learning better-than-random rotations from finite data is fundamentally difficult in high dimensions. In iterative training, algorithms often seek directions of maximal non-Gaussianity to rotate into coordinate axes. However, the authors observe that in high dimensions, \textit{spurious non-Gaussian projections} exist in any finite sample. This means that even if the true distribution is perfectly normal, a large dataset in $D$ dimensions will almost surely exhibit some random fluctuations that appear non-Gaussian. An algorithm that selects these directions may inadvertently overfit, ultimately increasing rather than reducing the overall KL loss \cite{draxler2023convergence}.
    
    \item \textbf{Dependency vs. Marginal Modelling}: The core reason for the slow scaling is identified as an architectural limitation—Gaussianization cannot capture inter-dimensional dependencies within a single layer. Each layer's nonlinear transform treats dimensions independently, reducing only \textit{marginal} non-Gaussianity but not correlations. Dependencies between dimensions (represented by the term $D$ in the KL decomposition) can only be mitigated by successive rotations over multiple layers. The paper argues that in high dimensions, this dependency structure dominates and is costly to eliminate \cite{draxler2023convergence}.

    \item \textbf{Empirical Findings and Favourable Cases}: Using both synthetic distributions and real-world datasets, the authors confirm that Gaussianization's layer complexity grows roughly linearly with $D$ in practice. However, they also find that certain distributions scale better. If additional dimensions are highly correlated or contain redundant information, the required number of layers may grow sub-linearly \cite{draxler2023convergence}. For example, in a constructed dataset beyond a certain image resolution, adding more pixels (dimensions) that are largely determined by others did not significantly increase the number of required layers. This suggests that Gaussianization can exploit inherent data structures, though the worst-case scaling remains linear.
\end{itemize}

The paper highlights that while Gaussianization is a universal but computationally expensive generative model in high dimensions, it struggles due to its inability to jointly model dependencies. Future research avenues proposed by the authors include designing better rotation strategies that avoid spurious effects while breaking dependencies more efficiently than random rotations \cite{draxler2023convergence}. Another direction is exploring hybrid models that incorporate coupling layers or other dependency-capturing mechanisms into Gaussianization, potentially combining the strengths of both approaches.

\section{A New Application of Gaussianization: Data Compression}

Beyond density estimation, we propose using Gaussianization flow for \textbf{lossless data compression}. The fundamental idea is that the method's ability to make data "as Gaussian as possible" can be exploited in compression. A multivariate normal distribution has maximal entropy among all distributions with the same covariance structure. By Gaussianizing data before compression, redundancies are reduced, leading to improved compression ratios.

Mathematically, consider a high-dimensional data source $X$ (e.g., images or audio signals). We train a Gaussianization flow $f(x)$ that maps $x \mapsto z = f(x)$, where $z$ follows an approximate $\mathcal{N}(0, I)$ distribution. The negative log-likelihood gives the optimal code length per datapoint in a compression scheme:

\begin{equation}
    -\log_2 p(x) = -\log_2 \left(| \det J_f(x)|^{-1} \prod_{i} \mathcal{N}(z_i; 0,1) \right),
\end{equation}

where $J_f(x)$ is the Jacobian determinant of the transformation. By minimising the KL divergence $D_{\mathit{KL}}(q(z) || \prod_i q(z_i))$, Gaussianization ensures that the latent components $z_i$ are as independent as possible, making the data more compressible \cite{draxler2023convergence}. This approach offers a novel method of transform coding, where an invertible flow preprocesses data into a form that can be encoded efficiently.

\section{Analysis of Theorem 1}

\textbf{Statement and Use Case:} Theorem 1 states that for a Gaussian input distribution $\mathcal{N}(0, \Sigma)$, at least $L \geq \frac{1}{2} (D+1)$ Gaussianization layers are required to exactly represent $p(x)$. This result applies in scenarios where one seeks to transform high-dimensional correlated Gaussian data into a standard normal form. A practical application is in \textit{whitening} or decorrelating high-dimensional Gaussian data using a Gaussianization flow.

\textbf{Significance and Novelty:} Theorem 1 establishes a fundamental lower bound on layer complexity, demonstrating that Gaussianization scales \textit{linearly} with dimension $D$. This is significant because it provides the first explicit quantification of Gaussianization's inefficiency in high dimensions. Prior to this work, universal approximation results guaranteed that such flows could model any distribution, but no concrete scaling laws were known.

\section{Significance of Key Results}

\textbf{Proposition 2:} The paper establishes that coupling-based normalizing flows can represent Gaussian distributions in a number of layers independent of $D$. This sharply contrasts with Gaussianization's $\Omega(D)$ complexity and highlights the advantage of models that capture dependencies across dimensions within each layer.

\textbf{Observation 1:} The authors observe that iteratively learning rotations by maximising marginal non-Gaussianity can lead to overfitting in high dimensions. They illustrate this with an example where a rotation produces a spurious bimodal marginal from standard normal data, demonstrating the risks of greedy optimisation in high-dimensional settings.

\textbf{Figure 3:} This figure provides a parameter-counting argument that visualises why Gaussianization scales poorly compared to coupling flows. It shows that Gaussianization must adjust each dimension separately, whereas coupling layers modify joint dependencies in fewer steps.
